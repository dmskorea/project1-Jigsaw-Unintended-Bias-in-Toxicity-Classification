merge baseline

baseline: 0.93845  / 7055s
https://www.kaggle.com/chechir/bert-lstm-rank-blender

1. ensemble

a. small bert model 120mil data : 0.93852 / 7110s
https://www.kaggle.com/kdafke123141/bert-lstm-rank-blender-small
maxlen을 300-> 220으로 올렸을 때 성능이 오른이유는 학습 셋이 220 으로 학습 시켰기 때문?
(박희준님 커널)

b. lstm + bert : small_max_300_120mi
0_epoch : 0.93899 ( frist)
1_epoch : 0.93957 ( second)
https://www.kaggle.com/kdafke123141/bert-lstm-rank-blender-small


c. lstm + bert : small_max_300_120mi (weight: 0.66 + 0.33)
1_epoch : 0.93975
https://www.kaggle.com/kdafke123141/bert-lstm-rank-blender-small-weight?scriptVersionId=15862427


d. jigsaw-starter-blend-base-max_300/epoch:2 : 0.93924


e. *lstm + bert : small_170mi_max_300_5e5
0_epoch : : 0.93913
1_epoch : 0.94056


f.  *lstm + bert : small_170mi-100mi_max_300_5e5-2e5
3_epoch : 0.935xx ( finetuing incorrect?)


g. 1 lstm + 2 bert ( differ lr) : 0.94076

h. 1 lstm + 3 bert ( differ lr , seed): 0.94086

i. 1 lstm + 3 bert + custom loss (weighted): 0.9424
  - lstm epoch 2 -> 3 
https://www.kaggle.com/kdafke123141/3bert-1stm-custom-loss

j. 1 lstm + 3 bert + custom loss + multi label cls : 0.94258






TODO
1. optimzer 변경 ( BertAdam -> standard adam)
2. Seed 변경하면서 lr 감쇠




2. valina

large
0.9285 (large maxlen220_epoch0)
0.9289 (large maxlen220_epoch1)

small 
0.9338 (samll maxlen300_epoch0)
0.9335 (samll maxlen300_epoch1)

small 170mil / maxlen300 / 5e5
0.9370  (epoch0 - loss 0.12)
0.9359  (epoch1 - loss 0.09 )


# single model

#vanali code test: 100K validation

//small
1. bert small single model : 0.93373  / (my own 120mil : 0.93413)
(vanila code test : 0.9298784415544122) : howto improve?

2. bert small single model_maxlen : ?
(vanila code test : epoch-0-> 0.9338 / epoch-1 -> 0.9335) : how to improve?


3. 150mi, maxlen 300, batch 64 seed 1234 : 0.934

4. small 150mil_max300_custom_loss
vanila code test : 0.9375

5. small 170mil_max300_custom_loss
vanila code test : 0.9403

//large
1. bert larger model : 0.92826
https://www.kaggle.com/kdafke123141/pytorch-bert-inference-large-model

2. bert larger model_epoch2 : 5 submition validation (after ...) 


/////////////////////////////////////////////////////////////////////////////////////////////

3차 모임 base line

data augmentation sorce 
(https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038#latest-310689)
-too many requested




baseline + data augmentation ( 이전 대회 데이터 활용)

https://www.kaggle.com/kdafke123141/pretext-lstm-tuning-augmentation-except-for-ori/edit/run/15342750

